<div align="center">

# ğŸ›’ Market Scraper

**Enterprise-grade data platform for supermarket price intelligence**

[![DBT Version](https://img.shields.io/badge/dbt-1.9+-blue.svg)](https://docs.getdbt.com/)
[![Python](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![DuckDB](https://img.shields.io/badge/duckdb-1.1+-yellow.svg)](https://duckdb.org/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Code Style](https://img.shields.io/badge/code%20style-sqlfluff-brightgreen.svg)](https://sqlfluff.com/)

[Features](#-features) â€¢
[Quick Start](#-quick-start) â€¢
[Architecture](#-architecture) â€¢
[Documentation](#-documentation) â€¢
[Contributing](#-contributing)

</div>

---

## ğŸ“‹ Overview

**Market Scraper** is a production-ready data platform that collects, transforms, and analyzes product pricing data from supermarket chains in FlorianÃ³polis, Brazil. Built with modern data engineering best practices, it provides real-time price intelligence for competitive analysis, market research, and consumer insights.

### Key Highlights

- ğŸª **Multi-Store Coverage**: Bistek, Fort Atacadista, Giassi (37 regions, 30K+ products)
- ğŸ“Š **Medallion Architecture**: Bronze â†’ Silver â†’ Gold layers with DBT transformations
- âš¡ **Parquet-First**: 35x faster queries, 600x smaller storage vs JSONL
- ğŸ”„ **Incremental Processing**: 90% time savings with smart watermarking
- ğŸ“¸ **Historical Tracking**: SCD Type 2 snapshots for price trends
- ğŸ›¡ï¸ **Quality Enforced**: Automated linting (SQLFluff), testing (DBT), and CI/CD

---

## ğŸ¯ Features

### Data Collection
- âœ… **Config-driven scrapers** using single `VTEXScraper` class
- âœ… **Regional pricing** with VTEX segment cookies (city-level granularity)
- âœ… **Parallel execution** with thread-safe batching
- âœ… **Auto-retry logic** with exponential backoff
- âœ… **Metadata injection** for lineage tracking

### Data Transformation (DBT)
- âœ… **Layered modeling**: Staging (ephemeral) â†’ Trusted (tables) â†’ Marts (analytics)
- âœ… **Incremental models** with merge/append strategies
- âœ… **Data contracts** enforced via DBT contracts
- âœ… **Snapshots (SCD Type 2)** for historical price analysis
- âœ… **Dimensional modeling** following Kimball methodology

### Analytics & Observability
- âœ… **DuckDB analytics engine** (embedded OLAP, Parquet-native)
- âœ… **Streamlit dashboards** for business + operational metrics
- âœ… **Data quality tests** (uniqueness, freshness, business rules)
- âœ… **Run metadata tracking** in `runs.duckdb`

### Quality & Governance
- âœ… **SQL linting** (SQLFluff with DuckDB dialect)
- âœ… **YAML validation** (yamllint for DBT schemas)
- âœ… **Pre-commit hooks** (15+ automated checks)
- âœ… **CI/CD pipelines** (GitHub Actions on PRs)
- âœ… **Documentation coverage** (100% models + columns)

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11+
- Git
- DuckDB (via `pip install duckdb`)
- DBT Core 1.9+ with DuckDB adapter

### Installation

```bash
# 1. Clone repository
git clone https://github.com/alanludke/market-scraper.git
cd market-scraper

# 2. Install dependencies
pip install -r requirements.txt

# 3. Configure environment (Windows only)
# PowerShell (Recommended):
[System.Environment]::SetEnvironmentVariable('PYTHONUTF8', '1', 'User')
# Restart terminal

# 4. Install DBT packages
cd src/transform/dbt_project
dbt deps
```

### Run Your First Scrape

```bash
# Scrape one store (limited to 1000 products)
python cli.py scrape bistek --limit 1000

# Output: data/bronze/supermarket=bistek/**/*.parquet
```

### Transform with DBT

```bash
cd src/transform/dbt_project

# Parse and validate
dbt parse

# Run transformations
dbt run

# Test data quality
dbt test

# Generate documentation
dbt docs generate
dbt docs serve  # Opens at http://localhost:8080
```

### View Analytics

```bash
# Python CLI (from project root)
python cli.py analytics --days 7

# Or use Streamlit dashboard
streamlit run app.py
```

---

## ğŸ—ï¸ Architecture

### High-Level Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VTEX APIs (Bistek, Fort, Giassi)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ Scrape (Python)
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BRONZE LAYER (Parquet)                                     â”‚
â”‚  Raw data, partitioned by store/region/date                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ DBT Transform
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SILVER LAYER (DBT - Trusted)                               â”‚
â”‚  Deduplication, type casting, business logic                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ DBT Aggregate
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GOLD LAYER (DBT - Marts)                                   â”‚
â”‚  Facts & dimensions, analytics-ready                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ Query
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ANALYTICS (DuckDB + Streamlit)                             â”‚
â”‚  Dashboards, reports, price intelligence                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Technology Stack

| Layer              | Technology          | Purpose                          |
| ------------------ | ------------------- | -------------------------------- |
| **Ingestion**      | Python (requests)   | VTEX API scraping                |
| **Storage**        | Parquet (Snappy)    | Columnar storage (80% compression) |
| **Transformation** | DBT + DuckDB        | SQL-first, incremental ELT       |
| **Analytics**      | DuckDB + Pandas     | OLAP queries, aggregations       |
| **Visualization**  | Streamlit           | Interactive dashboards           |
| **Orchestration**  | Cron / Prefect      | Daily scheduling                 |
| **Quality**        | SQLFluff, yamllint  | Linting, validation              |
| **CI/CD**          | GitHub Actions      | Automated testing on PRs         |

### Data Layers (Medallion Architecture)

| Layer      | Materialization | Purpose                      | Example Model         |
| ---------- | --------------- | ---------------------------- | --------------------- |
| **Raw**    | External        | Unprocessed Parquet files    | `bronze_bistek.products` |
| **Staging** | Ephemeral      | Cast, rename, clean          | `stg_vtex__products`  |
| **Trusted** | Table          | Business logic, dedup        | `tru_product`         |
| **Marts**   | Table/Incremental | Analytics-ready facts/dims | `fct_prices_daily`    |

ğŸ“– **Deep dive**: [DATA_LAYERS.md](docs/architecture/DATA_LAYERS.md)

---

## ğŸ“š Documentation

### Getting Started
- ğŸ“– [SETUP.md](SETUP.md) - Installation & configuration guide
- ğŸ“– [src/transform/dbt_project/README.md](src/transform/dbt_project/README.md) - DBT quick reference

### Architecture
- ğŸ›ï¸ [ARCHITECTURE.md](docs/architecture/ARCHITECTURE.md) - System design & data flow
- ğŸ“Š [DATA_LAYERS.md](docs/architecture/DATA_LAYERS.md) - Medallion architecture (5 layers)
- ğŸ“¸ [SNAPSHOTS.md](docs/architecture/SNAPSHOTS.md) - Historical tracking (SCD Type 2)
- âš¡ [INCREMENTAL_MODELS.md](docs/architecture/INCREMENTAL_MODELS.md) - Performance optimization

### Templates
- ğŸ“‹ [EDA_TEMPLATE.md](docs/templates/EDA_TEMPLATE.md) - Exploratory data analysis checklist
- ğŸ“ˆ [KPI_MATRIX.md](docs/templates/KPI_MATRIX.md) - KPI documentation template
- âœ… [PR_CHECKLIST.md](docs/templates/PR_CHECKLIST.md) - Pull request review guide
- ğŸ§© [KIMBALL_BUS_MATRIX.md](docs/templates/KIMBALL_BUS_MATRIX.md) - Dimensional modeling
- ğŸ—‚ï¸ [LOGICAL_DATA_MODEL.md](docs/templates/LOGICAL_DATA_MODEL.md) - ERD template

### Development
- ğŸŒ³ [GIT_FLOW.md](docs/development/GIT_FLOW.md) - Branching strategy & workflow
- ğŸ§ª [TESTING_STRATEGY.md](docs/quality/TESTING_STRATEGY.md) - DBT testing guide
- ğŸ›¡ï¸ [PROJECT_QUALITY_STANDARDS.md](docs/quality/PROJECT_QUALITY_STANDARDS.md) - Quality enforcement

---

## ğŸ—‚ï¸ Project Structure

```
market-scraper/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingest/                     # Data extraction
â”‚   â”‚   â”œâ”€â”€ scrapers/               # VTEXScraper (config-driven)
â”‚   â”‚   â””â”€â”€ loaders/                # Parquet writer, Azure uploader
â”‚   â”œâ”€â”€ transform/                  # DBT project
â”‚   â”‚   â””â”€â”€ dbt_project/
â”‚   â”‚       â”œâ”€â”€ models/             # SQL transformations
â”‚   â”‚       â”‚   â”œâ”€â”€ staging/        # Ephemeral (stg_*)
â”‚   â”‚       â”‚   â”œâ”€â”€ trusted/        # Tables (tru_*)
â”‚   â”‚       â”‚   â””â”€â”€ marts/          # Facts/Dims (fct_*, dim_*)
â”‚   â”‚       â”œâ”€â”€ macros/             # Reusable SQL functions
â”‚   â”‚       â”œâ”€â”€ tests/              # Custom data tests
â”‚   â”‚       â”œâ”€â”€ .sqlfluff           # SQL linting config
â”‚   â”‚       â”œâ”€â”€ .yamllint           # YAML validation
â”‚   â”‚       â””â”€â”€ .pre-commit-config.yaml  # Git hooks
â”‚   â””â”€â”€ analytics/                  # Queries & dashboards
â”‚       â””â”€â”€ engine.py               # DuckDB query engine
â”œâ”€â”€ data/                           # All data files (gitignored)
â”‚   â”œâ”€â”€ bronze/                     # Raw Parquet (scrapers)
â”‚   â”œâ”€â”€ silver/                     # Cleaned (DBT)
â”‚   â”œâ”€â”€ gold/                       # Analytics (DBT)
â”‚   â”œâ”€â”€ logs/                       # Application logs
â”‚   â””â”€â”€ metrics/                    # Operational metadata
â”œâ”€â”€ docs/                           # Comprehensive documentation
â”‚   â”œâ”€â”€ architecture/               # System design
â”‚   â”œâ”€â”€ templates/                  # Reusable guides
â”‚   â”œâ”€â”€ development/                # Dev workflows
â”‚   â””â”€â”€ quality/                    # Testing & standards
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/                  # CI/CD pipelines
â”‚       â”œâ”€â”€ lint.yml                # SQL + YAML linting
â”‚       â””â”€â”€ test.yml                # DBT tests
â”œâ”€â”€ config/
â”‚   â””â”€â”€ stores.yaml                 # Store configurations
â”œâ”€â”€ cli.py                          # Main CLI interface
â”œâ”€â”€ app.py                          # Streamlit dashboard
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ CLAUDE.md                       # AI context file
â””â”€â”€ README.md                       # You are here!
```

---

## ğŸ’» Usage Examples

### Scraping

```bash
# Scrape all stores (parallel execution)
python cli.py scrape --all --parallel

# Scrape specific store with limit
python cli.py scrape bistek --limit 5000

# Scrape specific region
python cli.py scrape giassi --regions florianopolis_centro

# Verbose output
python cli.py -v scrape fort
```

### DBT Transformations

```bash
cd src/transform/dbt_project

# Run full pipeline
dbt run

# Run specific layer
dbt run --select staging.*
dbt run --select trusted.*
dbt run --select marts.*

# Run incrementally (smart refresh)
dbt run --select +tru_product+  # Model + upstream + downstream

# Run with full refresh
dbt run --full-refresh --select tru_product

# Test data quality
dbt test
dbt test --select tru_product

# Generate lineage docs
dbt docs generate && dbt docs serve
```

### Analytics Queries

```python
# Python API
from src.analytics.engine import AnalyticsEngine

engine = AnalyticsEngine()

# Price comparison
prices = engine.query("""
    SELECT
        supermarket,
        AVG(min_price) as avg_price
    FROM tru_product
    WHERE scraped_date = CURRENT_DATE
    GROUP BY supermarket
""")

# Historical trends (with snapshots)
trends = engine.query("""
    SELECT
        product_name,
        dbt_valid_from as price_date,
        min_price
    FROM snapshots.snap_product_prices
    WHERE product_id = '12345'
    ORDER BY dbt_valid_from DESC
""")
```

---

## ğŸ§ª Testing

### Run All Tests

```bash
# Python unit tests (if implemented)
pytest tests/

# DBT data tests
cd src/transform/dbt_project
dbt test

# SQL linting
sqlfluff lint models/ --dialect duckdb

# YAML validation
yamllint -c .yamllint models/
```

### Pre-commit Hooks

```bash
# Install hooks
cd src/transform/dbt_project
pip install pre-commit
pre-commit install

# Run manually
pre-commit run --all-files

# Run specific hook
pre-commit run sqlfluff-lint
```

### CI/CD

GitHub Actions workflows run automatically on PRs:
- âœ… SQL linting (SQLFluff)
- âœ… YAML validation (yamllint)
- âœ… DBT parsing (`dbt parse`)
- âœ… DBT compilation (`dbt compile`)

---

## ğŸ“Š Data Metrics

### Current Coverage (as of 2026-02-05)

| Store          | Regions | Products | Daily Rows | Historical Data |
| -------------- | ------- | -------- | ---------- | --------------- |
| Bistek         | 13      | ~10K     | 130K       | 11GB            |
| Fort Atacadista| 7       | ~10K     | 70K        | 11GB            |
| Giassi         | 17      | ~10K     | 170K       | 11GB            |
| **Total**      | **37**  | **30K**  | **370K**   | **~33GB**       |

### Performance Gains

| Metric              | Before (JSONL) | After (Parquet) | Improvement |
| ------------------- | -------------- | --------------- | ----------- |
| **Storage**         | 11GB           | 18MB            | 600x smaller |
| **Query Time**      | 60s            | 1.7s            | 35x faster  |
| **ETL Duration**    | 50s (full)     | 5s (incremental)| 90% faster  |

---

## ğŸ› ï¸ Development

### Prerequisites for Contributors

- Python 3.11+
- Git
- DBT Core 1.9+
- SQLFluff (for linting)
- Pre-commit (for hooks)

### Development Workflow

1. **Create feature branch**:
   ```bash
   git checkout -b feature/add-new-store
   ```

2. **Make changes** following [GIT_FLOW.md](docs/development/GIT_FLOW.md)

3. **Run quality checks**:
   ```bash
   cd src/transform/dbt_project
   pre-commit run --all-files
   dbt test
   ```

4. **Create PR** using [PR_CHECKLIST.md](docs/templates/PR_CHECKLIST.md)

5. **Wait for CI/CD** to pass (automated)

6. **Merge** after approval

### Code Standards

- âœ… **SQL**: Lowercase keywords, 4-space indent, leading commas
- âœ… **Python**: PEP 8 (black formatter)
- âœ… **YAML**: 2-space indent, 250 char max line
- âœ… **Naming**: `stg_*`, `tru_*`, `fct_*`, `dim_*` conventions
- âœ… **Documentation**: 100% coverage (models + columns)

ğŸ“– **Details**: [PROJECT_QUALITY_STANDARDS.md](docs/quality/PROJECT_QUALITY_STANDARDS.md)

---

## ğŸ¤ Contributing

We welcome contributions! Please follow these steps:

1. **Fork** the repository
2. **Create** a feature branch (`feature/amazing-feature`)
3. **Commit** with clear messages
4. **Push** to your fork
5. **Open** a Pull Request

### Contribution Guidelines

- ğŸ“‹ Follow the [PR Checklist](docs/templates/PR_CHECKLIST.md)
- ğŸ§ª Add tests for new features
- ğŸ“– Update documentation
- âœ… Ensure CI/CD passes
- ğŸ” Request review from maintainers

---

## ğŸ“ˆ Roadmap

### Phase 1: Foundation âœ… (Completed)
- [x] Config-driven VTEX scrapers
- [x] Parquet-first storage
- [x] DBT project setup
- [x] Basic analytics (DuckDB)

### Phase 2: Quality & Testing âœ… (Completed)
- [x] SQLFluff linting
- [x] Pre-commit hooks
- [x] GitHub Actions CI/CD
- [x] Data quality tests

### Phase 3: Advanced Features ğŸš§ (In Progress)
- [ ] Incremental models (watermarking)
- [ ] Snapshots (SCD Type 2)
- [ ] Dimensional modeling (facts/dims)
- [ ] Streamlit operational dashboard

### Phase 4: Scale & Deploy ğŸ”® (Planned)
- [ ] Add 10+ stores
- [ ] Prefect orchestration
- [ ] Azure Blob sync
- [ ] Docker containerization

---

## ğŸ› Troubleshooting

### Common Issues

#### "DBT cannot open analytics.duckdb"
**Cause**: Database locked by another process.
**Fix**: Close Python scripts, DBeaver, or other connections.

#### "UnicodeDecodeError on Windows"
**Cause**: Missing UTF-8 encoding.
**Fix**: Set `PYTHONUTF8=1` environment variable ([SETUP.md](SETUP.md))

#### "SQLFluff lint failed"
**Cause**: SQL doesn't follow style guide.
**Fix**: Run `sqlfluff fix models/<model>.sql`

ğŸ“– **More solutions**: [PROJECT_QUALITY_STANDARDS.md](docs/quality/PROJECT_QUALITY_STANDARDS.md#troubleshooting)

---

## ğŸ“„ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **DBT** - SQL-first transformation framework
- **DuckDB** - Blazing-fast embedded OLAP
- **SQLFluff** - SQL linting for data teams
- **Kimball Group** - Dimensional modeling methodology
- **VTEX** - E-commerce platform APIs

---

## ğŸ“ Contact

- **Author**: Alan Ludke
- **GitHub**: [@alanludke](https://github.com/alanludke)
- **Project**: [market-scraper](https://github.com/alanludke/market-scraper)

---

<div align="center">

**Made with â¤ï¸ for the data community**

[â­ Star this repo](https://github.com/alanludke/market-scraper) if you find it useful!

</div>
