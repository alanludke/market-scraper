# Guia de Migra√ß√£o - Dados Legados

Este guia explica como migrar os dados do `/archive` para a estrutura atual do projeto.

## Vis√£o Geral

O script de migra√ß√£o (`scripts/migrate_legacy_data.py`) converte dados JSONL legados para Parquet com:

‚úÖ **Valida√ß√£o de schema** - Valida todos os registros contra `VTEXProduct` (Pydantic)
‚úÖ **Limpeza de dados** - Normaliza campos (pre√ßos, EANs, etc.)
‚úÖ **Deduplica√ß√£o** - Remove duplicatas por `product_id + scraped_at`
‚úÖ **Compress√£o** - Converte JSONL ‚Üí Parquet (redu√ß√£o de ~80-90%)
‚úÖ **Estrutura correta** - Segue naming convention atual (`run_{store}_{timestamp}`)

## Estrutura

### Origem (Archive)
```
archive/legacy_scrapers/
‚îú‚îÄ‚îÄ bistek_products_scraper/
‚îÇ   ‚îî‚îÄ‚îÄ data/bronze/supermarket=bistek/region=balneario_camboriu/
‚îÇ       ‚îî‚îÄ‚îÄ year=2026/month=01/day=25/run_20260125_161503/
‚îÇ           ‚îî‚îÄ‚îÄ bistek_balneario_camboriu_full.jsonl
‚îú‚îÄ‚îÄ fort_products_scraper/
‚îî‚îÄ‚îÄ giassi_products_scraper/
```

### Destino (Bronze)
```
data/bronze/
‚îî‚îÄ‚îÄ supermarket=bistek/region=balneario_camboriu/
    ‚îî‚îÄ‚îÄ year=2026/month=01/day=25/
        ‚îî‚îÄ‚îÄ run_bistek_20260125_161503.parquet
```

## Como Usar

### 1. Dry Run (Testar sem escrever)

Recomendado para verificar quantos registros ser√£o migrados:

```bash
# Testar uma loja
python scripts/migrate_legacy_data.py --store bistek --dry-run

# Testar todas as lojas
python scripts/migrate_legacy_data.py --store all --dry-run
```

### 2. Migra√ß√£o com Filtro de Data

Migrar apenas dados de um per√≠odo espec√≠fico:

```bash
# Migrar dados de Janeiro/2026
python scripts/migrate_legacy_data.py \
    --store bistek \
    --start-date 2026-01-25 \
    --end-date 2026-01-31

# Migrar tudo at√© uma data
python scripts/migrate_legacy_data.py \
    --store all \
    --end-date 2026-02-01
```

### 3. Migra√ß√£o Completa

Migrar **todos** os dados de todas as lojas:

```bash
# ‚ö†Ô∏è ATEN√á√ÉO: Isso processar√° ~138 arquivos JSONL (~12GB)
python scripts/migrate_legacy_data.py --store all
```

### 4. Migra√ß√£o por Loja

Migrar uma loja espec√≠fica:

```bash
# Bistek (m√∫ltiplas regi√µes)
python scripts/migrate_legacy_data.py --store bistek

# Fort (Florian√≥polis)
python scripts/migrate_legacy_data.py --store fort

# Giassi (m√∫ltiplas lojas)
python scripts/migrate_legacy_data.py --store giassi
```

## O Que o Script Faz

### 1. Leitura e Valida√ß√£o
```python
# L√™ JSONL linha por linha
for line in jsonl_file:
    record = json.loads(line)

    # Valida com Pydantic schema
    product = VTEXProduct.parse_obj(record)

    # Registros inv√°lidos s√£o descartados e contabilizados
```

### 2. Limpeza e Normaliza√ß√£o
```python
# VTEXProduct schema normaliza:
# - Pre√ßos (Decimal)
# - EANs (string, leading zeros)
# - Timestamps (datetime)
# - product_id (string)
# - Campos opcionais (None se ausente)
```

### 3. Deduplica√ß√£o
```python
# Remove duplicatas por chave composta
df.drop_duplicates(subset=["product_id", "scraped_at"], keep="first")
```

### 4. Convers√£o para Parquet
```python
df.to_parquet(
    output_file,
    engine="pyarrow",
    compression="snappy",  # Compress√£o r√°pida e eficiente
    index=False
)
```

## Estat√≠sticas de Sa√≠da

O script exibe estat√≠sticas detalhadas:

```
============================================================
MIGRATION SUMMARY
============================================================
Files processed: 138
Files skipped: 2
Records total: 1,234,567
Records migrated: 1,200,000
Records invalid: 34,567  (2.8%)
Records duplicated: 0
Errors: 0
Success rate: 97.2%
============================================================
```

### Logs Detalhados

Logs salvos em `data/logs/migration_{timestamp}.log`:

```json
{
  "time": "2026-02-08 14:30:00",
  "level": "INFO",
  "message": "Processing: bistek_balneario_camboriu_full.jsonl"
}
{
  "time": "2026-02-08 14:30:05",
  "level": "INFO",
  "message": "Read 12,345 records"
}
{
  "time": "2026-02-08 14:30:10",
  "level": "INFO",
  "message": "Valid records: 12,100/12,345"
}
```

## Valida√ß√£o de Schema (VTEXProduct)

O script valida os seguintes campos obrigat√≥rios:

| Campo | Tipo | Valida√ß√£o |
|-------|------|-----------|
| `product_id` | string | Obrigat√≥rio, n√£o vazio |
| `product_name` | string | Obrigat√≥rio, n√£o vazio |
| `brand` | string | Obrigat√≥rio |
| `ean` | string | Obrigat√≥rio, 13 d√≠gitos |
| `price` | Decimal | > 0 |
| `list_price` | Decimal | ‚â• price |
| `available` | bool | true/false |
| `category_id` | string | Obrigat√≥rio |
| `link` | AnyUrl | URL v√°lida |
| `image_url` | AnyUrl | URL v√°lida |
| `scraped_at` | datetime | ISO 8601 |

**Registros inv√°lidos** s√£o descartados e contabilizados em `records_invalid`.

## Tratamento de Erros

### Erro: "Legacy directory not found"
```bash
# Verificar se o diret√≥rio existe
ls archive/legacy_scrapers/

# Solu√ß√£o: Verifique se o archive foi extra√≠do corretamente
```

### Erro: "No valid records after validation"
```bash
# Arquivo JSONL corrompido ou schema incompat√≠vel
# Solu√ß√£o: Verificar logs detalhados em data/logs/migration_*.log
```

### Erro: "Invalid JSON line"
```bash
# Linha corrompida no JSONL
# Solu√ß√£o: Script pula linhas inv√°lidas automaticamente
```

## Verifica√ß√£o P√≥s-Migra√ß√£o

### 1. Verificar arquivos Parquet criados
```bash
# Contar arquivos Parquet
find data/bronze -name "*.parquet" | wc -l

# Verificar tamanho total
du -sh data/bronze
```

### 2. Testar leitura com DuckDB
```bash
python -c "
import duckdb
con = duckdb.connect()
print(con.execute('''
    SELECT
        COUNT(*) as total_products,
        COUNT(DISTINCT product_id) as unique_products,
        MIN(scraped_at) as first_scrape,
        MAX(scraped_at) as last_scrape
    FROM read_parquet(\"data/bronze/**/*.parquet\")
''').fetchall())
"
```

### 3. Executar DBT para processar bronze ‚Üí silver
```bash
cd src/transform/dbt_project

# Processar staging (bronze ‚Üí silver)
dbt run --select staging.*

# Verificar dados
dbt test --select staging.*
```

## Limpeza do Archive

‚ö†Ô∏è **ATEN√á√ÉO**: Apenas delete o `/archive` **AP√ìS** confirmar que:

1. ‚úÖ Migra√ß√£o completa sem erros
2. ‚úÖ DBT processou dados com sucesso
3. ‚úÖ Dashboards funcionando com novos dados
4. ‚úÖ Backup do archive em local seguro (se necess√°rio)

```bash
# Fazer backup do archive (opcional)
tar -czf archive_backup_$(date +%Y%m%d).tar.gz archive/

# Verificar tamanho do backup
ls -lh archive_backup_*.tar.gz

# Deletar archive (irrevers√≠vel!)
rm -rf archive/
```

## Compara√ß√£o: Antes vs Depois

### Antes (Archive JSONL)
- üìÅ 138 arquivos JSONL
- üíæ 12 GB de dados
- ‚ùå Sem valida√ß√£o de schema
- ‚ùå Registros duplicados
- ‚ùå Campos inconsistentes
- ‚è±Ô∏è Queries lentas (60s+)

### Depois (Bronze Parquet)
- üìÅ 138 arquivos Parquet
- üíæ ~1.2 GB de dados (90% redu√ß√£o)
- ‚úÖ Schema validado (VTEXProduct)
- ‚úÖ Dados deduplicados
- ‚úÖ Campos normalizados
- ‚ö° Queries r√°pidas (<2s)

## Troubleshooting

### Progress Bar n√£o aparece
```bash
# Instalar tqdm
pip install tqdm
```

### Erro de import VTEXProduct
```bash
# Instalar depend√™ncias
pip install -r requirements.txt

# Verificar se src/ est√° no PYTHONPATH
export PYTHONPATH="${PYTHONPATH}:$(pwd)"
```

### Espa√ßo em disco insuficiente
```bash
# Verificar espa√ßo dispon√≠vel
df -h

# Limpar arquivos tempor√°rios
rm -rf data/logs/migration_*.log
```

## FAQ

**Q: Preciso migrar tudo de uma vez?**
A: N√£o. Voc√™ pode migrar por loja ou por per√≠odo de data.

**Q: Os dados originais s√£o modificados?**
A: N√£o. O script apenas **l√™** do `/archive` e **escreve** em `data/bronze/`.

**Q: Posso rodar a migra√ß√£o m√∫ltiplas vezes?**
A: Sim, mas arquivos duplicados sobrescrever√£o os anteriores (mesmo `run_id`).

**Q: O que acontece com registros inv√°lidos?**
A: S√£o descartados e contabilizados em `records_invalid` nas estat√≠sticas.

**Q: Preciso rodar DBT depois?**
A: Sim! A migra√ß√£o apenas move para bronze. DBT processa bronze ‚Üí silver ‚Üí gold.

**Q: Posso cancelar a migra√ß√£o?**
A: Sim (Ctrl+C). Arquivos j√° migrados permanecer√£o em `data/bronze/`.

## Pr√≥ximos Passos

Ap√≥s a migra√ß√£o:

1. **Executar DBT** para processar os dados migrados:
   ```bash
   cd src/transform/dbt_project
   dbt run --select staging.*
   dbt run --select trusted.*
   dbt run --select marts.*
   ```

2. **Validar qualidade** com Great Expectations:
   ```bash
   great_expectations checkpoint run bronze_checkpoint
   ```

3. **Testar dashboards** com os novos dados:
   ```bash
   streamlit run src/dashboard/app.py
   ```

4. **Fazer backup e deletar archive** (opcional):
   ```bash
   tar -czf archive_backup.tar.gz archive/
   rm -rf archive/
   ```

---

**√öltima atualiza√ß√£o**: 2026-02-08
**Autor**: Market Scraper Team
