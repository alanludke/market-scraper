# Guia de Otimiza√ß√µes - Scraping Performance

## üìä Resumo das Otimiza√ß√µes Implementadas

Data: 2026-02-07
Vers√£o: 1.0

### Mudan√ßas Realizadas

#### 1Ô∏è‚É£ Scraping Incremental (Maior Impacto!)

**Arquivo modificado**: `src/orchestration/scraper_flow.py`

**O que mudou**:
- ‚úÖ Adicionado par√¢metro `use_incremental` (default: `True`)
- ‚úÖ Adicionado par√¢metro `incremental_days` (default: `7`)
- ‚úÖ Flow automaticamente passa flag `--incremental` para CLI

**Como usar**:

```python
# Run di√°ria (incremental - apenas √∫ltimos 7 dias)
daily_scraper_flow()

# Full scraping mensal (cat√°logo completo)
daily_scraper_flow(use_incremental=False)

# Incremental customizado (√∫ltimos 14 dias)
daily_scraper_flow(incremental_days=14)
```

**Ganho esperado**: **8-16x mais r√°pido** (8h ‚Üí 30-60 min)

---

#### 2Ô∏è‚É£ Request Delay Otimizado

**Arquivo modificado**: `config/stores.yaml`

**Mudan√ßas**:
- Angeloni: `request_delay: 0.3` ‚Üí `0.1` (3x mais r√°pido)
- Carrefour: `request_delay: 0.3` ‚Üí `0.1` (3x mais r√°pido)
- SuperKoch: `request_delay: 0.3` ‚Üí `0.1` (3x mais r√°pido)

**Justificativa**:
- APIs VTEX (Bistek, Fort, Giassi) j√° usam 0.1s com sucesso
- Reduz tempo total sem causar rate limiting
- Compat√≠vel com boas pr√°ticas de scraping

**Ganho esperado**: **3x mais r√°pido** (8h ‚Üí 2.7h para full scraping)

---

#### 3Ô∏è‚É£ Batch Size Aumentado

**Arquivo modificado**: `config/stores.yaml`

**Mudan√ßas**:
- Angeloni: `batch_size: 20` ‚Üí `50`
- Carrefour: `batch_size: 20` ‚Üí `50`
- SuperKoch: `batch_size: 20` ‚Üí `50`

**Benef√≠cios**:
- Menos arquivos Parquet gerados (498 ‚Üí ~200)
- Reduz overhead de I/O
- Facilita processamento downstream (DBT)

**Ganho esperado**: **10-15% mais r√°pido**

---

## üéØ Resultados Esperados

### Performance Comparativa

| M√©trica | ANTES | DEPOIS | Melhoria |
|---------|-------|--------|----------|
| **Full Scraping** | 8h | 2.7h | **3x mais r√°pido** |
| **Incremental (7d)** | N/A | 30-60 min | **8-16x vs full** |
| **Arquivos/batch** | 498 | ~200 | **60% menos arquivos** |
| **Overhead I/O** | Alto | Baixo | **10-15% redu√ß√£o** |

### Cen√°rios de Uso

#### üìÖ Runs Di√°rias (Recomendado: Incremental)

```bash
# Via Prefect
prefect deployment run daily-scraper/daily-scraper

# Ou via Python
python src/orchestration/scraper_flow.py
```

**Configura√ß√£o**:
- Modo: Incremental (√∫ltimos 7 dias)
- Tempo estimado: **30-60 minutos**
- Produtos: ~500-1,000 (apenas novos/modificados)
- Economia: **~85% do tempo**

#### üîÑ Runs Mensais (Full Refresh)

```bash
# Via Prefect (passar par√¢metro via JSON)
prefect deployment run daily-scraper/daily-scraper \
  --param use_incremental=false

# Ou via Python
python -c "from src.orchestration.scraper_flow import daily_scraper_flow; daily_scraper_flow(use_incremental=False)"
```

**Configura√ß√£o**:
- Modo: Full catalog
- Tempo estimado: **2.7 horas**
- Produtos: ~10,000 (cat√°logo completo)
- Redu√ß√£o: **8h ‚Üí 2.7h (3x mais r√°pido)**

---

## üß™ Como Testar

### 1. Teste R√°pido (Incremental com Limite)

```bash
# Teste com apenas 100 produtos (validar que --incremental funciona)
python scripts/cli.py scrape angeloni --incremental 7 --limit 100
```

**Valida√ß√£o esperada**:
- ‚úÖ Log mostra "Incremental discovery" com data de corte
- ‚úÖ Apenas produtos modificados nos √∫ltimos 7 dias s√£o descobertos
- ‚úÖ Tempo: ~5-10 minutos para 100 produtos

### 2. Teste Full (Uma Loja)

```bash
# Teste full scraping de uma loja com novas configs
python scripts/cli.py scrape angeloni --region florianopolis_centro
```

**Valida√ß√£o esperada**:
- ‚úÖ `request_delay` de 0.1s est√° sendo respeitado
- ‚úÖ Batches com ~50 produtos (n√£o 20)
- ‚úÖ Tempo reduzido ~3x comparado com runs anteriores

### 3. Teste Prefect Flow (Incremental)

```bash
# Rodar flow incremental localmente
python src/orchestration/scraper_flow.py
```

**Valida√ß√£o esperada**:
- ‚úÖ Flag `--incremental 7` √© passada para CLI
- ‚úÖ Log mostra "INCREMENTAL (last 7 days)"
- ‚úÖ Resumo final mostra "Time saved: ~85%"

---

## üìä Monitoramento

### M√©tricas a Acompanhar

1. **Tempo de Execu√ß√£o**
   ```bash
   # Verificar dura√ß√£o das runs no Prefect Dashboard
   # http://127.0.0.1:4200
   ```

2. **Produtos Descobertos**
   ```bash
   # Comparar incremental vs full
   # Incremental deveria descobrir ~10-20% do cat√°logo
   ```

3. **Rate Limiting (HTTP 429)**
   ```bash
   # Monitorar logs para erros 429
   tail -f data/logs/app.log | grep -i "429\|rate limit"
   ```

4. **Tamanho dos Batches**
   ```bash
   # Verificar que batches t√™m ~50 produtos
   python -c "import duckdb; conn = duckdb.connect('data/analytics.duckdb'); print(conn.execute('SELECT COUNT(*) FROM read_parquet(\"data/bronze/supermarket=angeloni/**/batches/batch_00001.parquet\")').fetchone())"
   ```

---

## ‚ö†Ô∏è Poss√≠veis Problemas e Solu√ß√µes

### Problema 1: Rate Limiting (HTTP 429)

**Sintoma**: Logs mostram erros "429 Too Many Requests"

**Solu√ß√£o**:
```yaml
# Em config/stores.yaml, aumentar delay
request_delay: 0.15  # ou 0.2
```

### Problema 2: Incremental n√£o encontra produtos

**Sintoma**: Scraping incremental descobre 0 produtos

**Causa**: Nenhum produto foi modificado nos √∫ltimos 7 dias

**Solu√ß√£o**:
```bash
# Aumentar janela temporal
python scripts/cli.py scrape angeloni --incremental 14
```

### Problema 3: Batches muito grandes (OOM)

**Sintoma**: Erro de mem√≥ria ao processar batches

**Solu√ß√£o**:
```yaml
# Em config/stores.yaml, reduzir batch_size
batch_size: 30  # em vez de 50
```

---

## üìÖ Roadmap Futuro (Fase 2 e 3)

### Fase 2: Paraleliza√ß√£o (1-2 meses)
- [ ] Converter `requests` ‚Üí `aiohttp` (async/await)
- [ ] Processar 5-10 produtos simultaneamente
- [ ] Ganho estimado: **3-5x mais r√°pido** (2.7h ‚Üí ~45 min)

### Fase 3: Cache Avan√ßado (2-3 meses)
- [ ] Hash-based deduplication
- [ ] Smart retry com cache local
- [ ] Ganho estimado: Incremental ‚Üí **15-20 min**

---

## üìö Refer√™ncias

- [CLAUDE.md](CLAUDE.md) - Arquitetura geral
- [src/orchestration/scraper_flow.py](src/orchestration/scraper_flow.py) - C√≥digo do flow
- [config/stores.yaml](config/stores.yaml) - Configura√ß√µes de delays e batches
- [src/ingest/scrapers/angeloni_html.py](src/ingest/scrapers/angeloni_html.py) - Implementa√ß√£o do scraping incremental

---

**√öltima atualiza√ß√£o**: 2026-02-07
**Vers√£o**: 1.0
**Autor**: Claude Code (Otimiza√ß√£o Fase 1)
